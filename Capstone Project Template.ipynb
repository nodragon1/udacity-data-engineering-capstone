{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Project Title\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "--describe your project at a high level--\n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Do all imports and installs here\n",
    "import pandas as pd\n",
    "from pyspark.sql import functions as func\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "In this project, we will gather two datasets(immigration and temperature), and check the correlation between immigration and temperature.\n",
    "\n",
    "#### Describe and Gather Data \n",
    "Immigration dataset comes from the US National Tourism and Trade Office, temperature dataset comes from Kaggle.\n",
    "Below is the detail about the datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Immigration data\n",
    "* i94yr: 4 numeric digit code of year\n",
    "* i94mon: numeric month\n",
    "* i94cit: 3 numeric digit code of origin country\n",
    "* i94port: 3 charactor code of destination city\n",
    "* arrdate: date of arrival to USA\n",
    "* depdate: date of departure from USA\n",
    "* i94visa: purpose of immigration (1=Buisness, 2=Pleasure, 3=Student)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# we need to build the spark session\n",
    "spark = SparkSession.builder\\\n",
    "        .config(\"spark.jars.repositories\", \"https://repos.spark-packages.org/\")\\\n",
    "        .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:2.7.0,saurfang:spark-sas7bdat:2.0.0-s_2.11\")\\\n",
    "        .enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_immig = spark.read.format('com.github.saurfang.sas.spark').load('../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3096313"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Immigration data has 3096313 records\n",
    "df_immig.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cicid: double (nullable = true)\n",
      " |-- i94yr: double (nullable = true)\n",
      " |-- i94mon: double (nullable = true)\n",
      " |-- i94cit: double (nullable = true)\n",
      " |-- i94res: double (nullable = true)\n",
      " |-- i94port: string (nullable = true)\n",
      " |-- arrdate: double (nullable = true)\n",
      " |-- i94mode: double (nullable = true)\n",
      " |-- i94addr: string (nullable = true)\n",
      " |-- depdate: double (nullable = true)\n",
      " |-- i94bir: double (nullable = true)\n",
      " |-- i94visa: double (nullable = true)\n",
      " |-- count: double (nullable = true)\n",
      " |-- dtadfile: string (nullable = true)\n",
      " |-- visapost: string (nullable = true)\n",
      " |-- occup: string (nullable = true)\n",
      " |-- entdepa: string (nullable = true)\n",
      " |-- entdepd: string (nullable = true)\n",
      " |-- entdepu: string (nullable = true)\n",
      " |-- matflag: string (nullable = true)\n",
      " |-- biryear: double (nullable = true)\n",
      " |-- dtaddto: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- insnum: string (nullable = true)\n",
      " |-- airline: string (nullable = true)\n",
      " |-- admnum: double (nullable = true)\n",
      " |-- fltno: string (nullable = true)\n",
      " |-- visatype: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_immig.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+\n",
      "|cicid| i94yr|i94mon|i94cit|i94res|i94port|arrdate|i94mode|i94addr|depdate|i94bir|i94visa|count|dtadfile|visapost|occup|entdepa|entdepd|entdepu|matflag|biryear| dtaddto|gender|insnum|airline|        admnum|fltno|visatype|\n",
      "+-----+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+\n",
      "|  6.0|2016.0|   4.0| 692.0| 692.0|    XXX|20573.0|   null|   null|   null|  37.0|    2.0|  1.0|    null|    null| null|      T|   null|      U|   null| 1979.0|10282016|  null|  null|   null| 1.897628485E9| null|      B2|\n",
      "|  7.0|2016.0|   4.0| 254.0| 276.0|    ATL|20551.0|    1.0|     AL|   null|  25.0|    3.0|  1.0|20130811|     SEO| null|      G|   null|      Y|   null| 1991.0|     D/S|     M|  null|   null|  3.73679633E9|00296|      F1|\n",
      "| 15.0|2016.0|   4.0| 101.0| 101.0|    WAS|20545.0|    1.0|     MI|20691.0|  55.0|    2.0|  1.0|20160401|    null| null|      T|      O|   null|      M| 1961.0|09302016|     M|  null|     OS|  6.66643185E8|   93|      B2|\n",
      "| 16.0|2016.0|   4.0| 101.0| 101.0|    NYC|20545.0|    1.0|     MA|20567.0|  28.0|    2.0|  1.0|20160401|    null| null|      O|      O|   null|      M| 1988.0|09302016|  null|  null|     AA|9.246846133E10|00199|      B2|\n",
      "| 17.0|2016.0|   4.0| 101.0| 101.0|    NYC|20545.0|    1.0|     MA|20567.0|   4.0|    2.0|  1.0|20160401|    null| null|      O|      O|   null|      M| 2012.0|09302016|  null|  null|     AA|9.246846313E10|00199|      B2|\n",
      "+-----+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Displays first 5 records\n",
    "df_immig.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Temperature data\n",
    "* AverageTemperature: average temperature\n",
    "* AverageTemperatureUncertainty: average temperature uncertainty\n",
    "* City: city name\n",
    "* Country: country name\n",
    "* Latitude: latitude\n",
    "* Longitude: longitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_temp = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"../../data2/GlobalLandTemperaturesByCity.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8599212"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Temperature data has 8599212 records\n",
    "df_temp.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dt: string (nullable = true)\n",
      " |-- AverageTemperature: string (nullable = true)\n",
      " |-- AverageTemperatureUncertainty: string (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- Latitude: string (nullable = true)\n",
      " |-- Longitude: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_temp.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+-----------------------------+-----+-------+--------+---------+\n",
      "|        dt|AverageTemperature|AverageTemperatureUncertainty| City|Country|Latitude|Longitude|\n",
      "+----------+------------------+-----------------------------+-----+-------+--------+---------+\n",
      "|1743-11-01|             6.068|           1.7369999999999999|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1743-12-01|              null|                         null|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1744-01-01|              null|                         null|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1744-02-01|              null|                         null|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1744-03-01|              null|                         null|Århus|Denmark|  57.05N|   10.33E|\n",
      "+----------+------------------+-----------------------------+-----+-------+--------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Displays first 5 records\n",
    "df_temp.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### Explore the Data \n",
    "We need to re-organize the data in some points\n",
    "1. Convert the sas file into dictionary form\n",
    "2. Make shared column between two datasets(i94port)\n",
    "3. Extract name of countries using i94cit column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Read the sas file which contains informations about city, country etc\n",
    "with open('I94_SAS_Labels_Descriptions.SAS') as f:\n",
    "    f_content = f.read()\n",
    "\n",
    "\n",
    "# This function convert sas file into dictionary form\n",
    "def code_mapper(file, idx):\n",
    "    f_content2 = f_content[f_content.index(idx):]\n",
    "    f_content2 = f_content2[:f_content2.index(';')].split('\\n')\n",
    "    f_content2 = [i.replace(\"'\", \"\") for i in f_content2]\n",
    "    dic = [i.split('=') for i in f_content2[1:]]\n",
    "    dic = dict([i[0].strip(), i[1].strip()] for i in dic if len(i) == 2)\n",
    "    return dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "i94prtl = code_mapper(f_content, \"i94prtl\") # city name dictionaty\n",
    "# i94prtl # Check what it looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "i94cntyl = code_mapper(f_content, \"i94cntyl\") # country name dictionary\n",
    "# i94cntyl # Check what it looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Dictionaries above don't look great as well.\n",
    "\n",
    "Values of city name dictionary contain informations about state which are not needed\n",
    "\n",
    "Keys of country name are string type but they are numbers and values are consisted of CAPITAL LETTERS\n",
    "'''\n",
    "\n",
    "code_dict = {}\n",
    "for key, value in i94prtl.items():\n",
    "    fields = value.split(', ')\n",
    "    code_dict[fields[0]] = key\n",
    "    \n",
    "country_dict = {}\n",
    "for key, value in i94cntyl.items():\n",
    "    lower = value[1:].lower()\n",
    "    country = value[0] + lower\n",
    "    country_dict[int(key)] = country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# we need functions for looking up city's name and country's name using codes\n",
    "def lookupCode(city):\n",
    "    try:\n",
    "        return code_dict[city]\n",
    "    except KeyError:\n",
    "        return None\n",
    "    \n",
    "def lookupCountry(i94cit):\n",
    "    try:\n",
    "        return country_dict[i94cit]\n",
    "    except KeyError:\n",
    "        return None\n",
    "    \n",
    "lookupCodeUDF = func.udf(lookupCode)\n",
    "lookupCountryUDF =func.udf(lookupCountry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+-----------------------------+-----+-------+--------+---------+-------+\n",
      "|        dt|AverageTemperature|AverageTemperatureUncertainty| City|Country|Latitude|Longitude|i94port|\n",
      "+----------+------------------+-----------------------------+-----+-------+--------+---------+-------+\n",
      "|1743-11-01|             6.068|           1.7369999999999999|Århus|Denmark|  57.05N|   10.33E|   null|\n",
      "|1743-12-01|              null|                         null|Århus|Denmark|  57.05N|   10.33E|   null|\n",
      "|1744-01-01|              null|                         null|Århus|Denmark|  57.05N|   10.33E|   null|\n",
      "|1744-02-01|              null|                         null|Århus|Denmark|  57.05N|   10.33E|   null|\n",
      "|1744-03-01|              null|                         null|Århus|Denmark|  57.05N|   10.33E|   null|\n",
      "+----------+------------------+-----------------------------+-----+-------+--------+---------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Add i94port column to temperature dataset\n",
    "df_temp = df_temp.withColumn(\"i94port\", lookupCodeUDF(func.upper(func.col(\"City\"))))\n",
    "df_temp.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+--------------+\n",
      "|cicid| i94yr|i94mon|i94cit|i94res|i94port|arrdate|i94mode|i94addr|depdate|i94bir|i94visa|count|dtadfile|visapost|occup|entdepa|entdepd|entdepu|matflag|biryear| dtaddto|gender|insnum|airline|        admnum|fltno|visatype|origin_country|\n",
      "+-----+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+--------------+\n",
      "|  6.0|2016.0|   4.0| 692.0| 692.0|    XXX|20573.0|   null|   null|   null|  37.0|    2.0|  1.0|    null|    null| null|      T|   null|      U|   null| 1979.0|10282016|  null|  null|   null| 1.897628485E9| null|      B2|       Ecuador|\n",
      "|  7.0|2016.0|   4.0| 254.0| 276.0|    ATL|20551.0|    1.0|     AL|   null|  25.0|    3.0|  1.0|20130811|     SEO| null|      G|   null|      Y|   null| 1991.0|     D/S|     M|  null|   null|  3.73679633E9|00296|      F1|          null|\n",
      "| 15.0|2016.0|   4.0| 101.0| 101.0|    WAS|20545.0|    1.0|     MI|20691.0|  55.0|    2.0|  1.0|20160401|    null| null|      T|      O|   null|      M| 1961.0|09302016|     M|  null|     OS|  6.66643185E8|   93|      B2|       Albania|\n",
      "| 16.0|2016.0|   4.0| 101.0| 101.0|    NYC|20545.0|    1.0|     MA|20567.0|  28.0|    2.0|  1.0|20160401|    null| null|      O|      O|   null|      M| 1988.0|09302016|  null|  null|     AA|9.246846133E10|00199|      B2|       Albania|\n",
      "| 17.0|2016.0|   4.0| 101.0| 101.0|    NYC|20545.0|    1.0|     MA|20567.0|   4.0|    2.0|  1.0|20160401|    null| null|      O|      O|   null|      M| 2012.0|09302016|  null|  null|     AA|9.246846313E10|00199|      B2|       Albania|\n",
      "+-----+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Add country's name column to immigration dataset\n",
    "df_immig = df_immig.withColumn('origin_country', lookupCountryUDF(func.col('i94cit')))\n",
    "df_immig.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+\n",
      "|     City|average_temp|\n",
      "+---------+------------+\n",
      "|  Antwerp|        9.88|\n",
      "| Araruama|       23.79|\n",
      "|Bangalore|       24.86|\n",
      "|    Benxi|        7.21|\n",
      "|Cajamarca|       16.89|\n",
      "+---------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "We need only one temperature information of each city\n",
    "\n",
    "But it's not good to pick randomly\n",
    "\n",
    "So we need to get average value of each city during the entire period\n",
    "'''\n",
    "average = df_temp.groupBy('City').agg(func.round(func.avg(\"AverageTemperature\"), 2).alias(\"average_temp\"))\n",
    "average.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+-----------------------------+-------+-------+--------+---------+-------+-------+------------+\n",
      "|        dt|AverageTemperature|AverageTemperatureUncertainty|   City|Country|Latitude|Longitude|i94port|   City|average_temp|\n",
      "+----------+------------------+-----------------------------+-------+-------+--------+---------+-------+-------+------------+\n",
      "|1743-11-01|              7.52|           1.6569999999999998|Antwerp|Belgium|  50.63N|    3.80E|   null|Antwerp|        9.88|\n",
      "|1743-12-01|              null|                         null|Antwerp|Belgium|  50.63N|    3.80E|   null|Antwerp|        9.88|\n",
      "|1744-01-01|              null|                         null|Antwerp|Belgium|  50.63N|    3.80E|   null|Antwerp|        9.88|\n",
      "|1744-02-01|              null|                         null|Antwerp|Belgium|  50.63N|    3.80E|   null|Antwerp|        9.88|\n",
      "|1744-03-01|              null|                         null|Antwerp|Belgium|  50.63N|    3.80E|   null|Antwerp|        9.88|\n",
      "+----------+------------------+-----------------------------+-------+-------+--------+---------+-------+-------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# We need to join the average table with the origianl one using city column\n",
    "df_temp = df_temp.alias('table1').join(average.alias('table2'), (func.col(\"table1.City\") == func.col(\"table2.City\")))\n",
    "df_temp.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Cleaning Steps\n",
    "Check empty, duplicated or useless datas and remove them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# We need to filter out records which is not about USA in temperature table, because immigration table only contains about USA \n",
    "df_temp = df_temp.filter(df_temp.Country == 'United States')\n",
    "\n",
    "# We need to filter out the records whose i94port is empty, because we have to join two tables with this column\n",
    "df_temp = df_temp.filter(df_temp.i94port != 'null')\n",
    "df_immig = df_immig.filter(df_immig.i94port != 'null')\n",
    "\n",
    "# We need to filter out the records whose i94port is 'XXX'\n",
    "df_immig = df_immig.filter(df_immig.i94port != 'XXX')\n",
    "\n",
    "# We need to remove duplicates\n",
    "df_temp = df_temp.dropDuplicates(['City'])\n",
    "df_immig = df_immig.dropDuplicates(['cicid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+---------------+-----+--------+--------------+\n",
      "|cicid| i94yr|i94mon|i94cit|i94res|i94port|arrdate|i94mode|i94addr|depdate|i94bir|i94visa|count|dtadfile|visapost|occup|entdepa|entdepd|entdepu|matflag|biryear| dtaddto|gender|insnum|airline|         admnum|fltno|visatype|origin_country|\n",
      "+-----+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+---------------+-----+--------+--------------+\n",
      "|299.0|2016.0|   4.0| 103.0| 103.0|    NYC|20545.0|    1.0|     NY|20550.0|  54.0|    2.0|  1.0|20160401|    null| null|      O|      O|   null|      M| 1962.0|06292016|  null|  null|     OS|5.5425872433E10|00087|      WT|       Austria|\n",
      "|305.0|2016.0|   4.0| 103.0| 103.0|    NYC|20545.0|    1.0|     NY|20555.0|  63.0|    2.0|  1.0|20160401|    null| null|      O|      O|   null|      M| 1953.0|06292016|  null|  null|     OS|5.5425817433E10|00087|      WT|       Austria|\n",
      "|496.0|2016.0|   4.0| 103.0| 103.0|    CHI|20545.0|    1.0|     IL|20548.0|  64.0|    1.0|  1.0|20160401|    null| null|      O|      O|   null|      M| 1952.0|06292016|  null|  null|     OS|5.5428623333E10|00065|      WB|       Austria|\n",
      "|558.0|2016.0|   4.0| 103.0| 103.0|    SFR|20545.0|    1.0|     CA|20547.0|  42.0|    1.0|  1.0|20160401|    null| null|      G|      O|   null|      M| 1974.0|06292016|     M|  null|     LH|5.5433311133E10|00454|      WB|       Austria|\n",
      "|596.0|2016.0|   4.0| 103.0| 103.0|    NAS|20545.0|    1.0|     FL|20547.0|  24.0|    2.0|  1.0|20160401|    null| null|      G|      N|   null|      M| 1992.0|06292016|     M|  null|     UP|5.5406105433E10|00221|      WT|       Austria|\n",
      "+-----+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+---------------+-----+--------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Final immigration table\n",
    "df_immig.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+-----------------------------+----------+-------------+--------+---------+-------+----------+------------+\n",
      "|        dt|AverageTemperature|AverageTemperatureUncertainty|      City|      Country|Latitude|Longitude|i94port|      City|average_temp|\n",
      "+----------+------------------+-----------------------------+----------+-------------+--------+---------+-------+----------+------------+\n",
      "|1743-11-01|            13.918|                        2.234|Charleston|United States|  32.95N|   79.47W|    CHS|Charleston|        18.7|\n",
      "|1835-01-01|            11.544|                        2.167|   Phoenix|United States|  32.95N|  112.02W|    PHO|   Phoenix|       21.05|\n",
      "|1743-11-01|            15.164|                        2.315|  Savannah|United States|  31.35N|   81.05W|    SAV|  Savannah|       19.41|\n",
      "|1758-03-01|             2.512|                        3.833|     Omaha|United States|  40.99N|   95.86W|    OMA|     Omaha|       10.05|\n",
      "|1828-01-01|           -17.738|                        3.468| Anchorage|United States|  61.88N|  151.13W|    ANC| Anchorage|        -2.3|\n",
      "+----------+------------------+-----------------------------+----------+-------------+--------+---------+-------+----------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Final temperature table\n",
    "df_temp.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "<center>\n",
    "  <img\n",
    "    src=\"erd.png\"\n",
    "    width=\"700\"\n",
    "    height=\"1400\"\n",
    "  />\n",
    "</center>\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "1. Gather data from sources\n",
    "2. Clean the data\n",
    "3. Select columns we need from temperature datasets to create dimesion temperature table\n",
    "4. Select columns we need from immigration datasets to create dimesion demographic table\n",
    "5. Join two datasets using i94port, select columns and create fact immigration table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create template view\n",
    "df_immig.createOrReplaceTempView('immigration')\n",
    "df_temp.createOrReplaceTempView('temperature')\n",
    "\n",
    "# Create dimension demographic table\n",
    "dim_demographic = df_immig.select('cicid', 'i94port', 'i94yr', 'i94mon', 'i94cit', 'arrdate', 'depdate', 'i94visa', 'origin_country')\n",
    "\n",
    "\n",
    "# Create dimension temperature table\n",
    "dim_temperature = df_temp.select('i94port', 'table1.City', 'Country', 'Latitude', 'Longitude', 'average_temp')\n",
    "\n",
    "\n",
    "# Create fact immigration table\n",
    "fact_immigration = spark.sql('''\n",
    "                             SELECT row_number() over (order by T.i94port) as immig_id,\n",
    "                             T.i94port, I.cicid, T.average_temp, I.i94yr, I.i94mon, I.i94cit, \n",
    "                             I.arrdate, I.depdate, I.i94visa\n",
    "                             FROM immigration I JOIN temperature T\n",
    "                             ON I.i94port = T.i94port\n",
    "                             ''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+------+------+------+-------+-------+-------+--------------+\n",
      "|cicid|i94port| i94yr|i94mon|i94cit|arrdate|depdate|i94visa|origin_country|\n",
      "+-----+-------+------+------+------+-------+-------+-------+--------------+\n",
      "|299.0|    NYC|2016.0|   4.0| 103.0|20545.0|20550.0|    2.0|       Austria|\n",
      "|305.0|    NYC|2016.0|   4.0| 103.0|20545.0|20555.0|    2.0|       Austria|\n",
      "|496.0|    CHI|2016.0|   4.0| 103.0|20545.0|20548.0|    1.0|       Austria|\n",
      "|558.0|    SFR|2016.0|   4.0| 103.0|20545.0|20547.0|    1.0|       Austria|\n",
      "|596.0|    NAS|2016.0|   4.0| 103.0|20545.0|20547.0|    2.0|       Austria|\n",
      "+-----+-------+------+------+------+-------+-------+-------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dim_demographic.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+-------------+--------+---------+------------+\n",
      "|i94port|      City|      Country|Latitude|Longitude|average_temp|\n",
      "+-------+----------+-------------+--------+---------+------------+\n",
      "|    CHS|Charleston|United States|  32.95N|   79.47W|        18.7|\n",
      "|    PHO|   Phoenix|United States|  32.95N|  112.02W|       21.05|\n",
      "|    SAV|  Savannah|United States|  31.35N|   81.05W|       19.41|\n",
      "|    OMA|     Omaha|United States|  40.99N|   95.86W|       10.05|\n",
      "|    ANC| Anchorage|United States|  61.88N|  151.13W|        -2.3|\n",
      "+-------+----------+-------------+--------+---------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dim_temperature.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+---------+------------+------+------+------+-------+-------+-------+\n",
      "|immig_id|i94port|    cicid|average_temp| i94yr|i94mon|i94cit|arrdate|depdate|i94visa|\n",
      "+--------+-------+---------+------------+------+------+------+-------+-------+-------+\n",
      "|       1|    ABQ|5289835.0|       11.14|2016.0|   4.0| 266.0|20572.0|   null|    3.0|\n",
      "|       2|    ABQ|5357011.0|       11.14|2016.0|   4.0| 582.0|20572.0|20642.0|    3.0|\n",
      "|       3|    ABQ|5289836.0|       11.14|2016.0|   4.0| 266.0|20572.0|20590.0|    3.0|\n",
      "|       4|    ANC|4310664.0|        -2.3|2016.0|   4.0| 209.0|20567.0|20588.0|    1.0|\n",
      "|       5|    ANC|3504250.0|        -2.3|2016.0|   4.0| 135.0|20563.0|20640.0|    2.0|\n",
      "+--------+-------+---------+------------+------+------+------+-------+-------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fact_immigration.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Save the tables as parquet file\n",
    "dim_demographic.write.mode('overwrite').partitionBy(\"i94port\").parquet(\"/sample/dim_demographic.parquet\")\n",
    "dim_temperature.write.mode('overwrite').parquet(\"/sample/dim_temperature.parquet\")\n",
    "fact_immigration.write.mode('overwrite').partitionBy(\"i94port\").parquet(\"/sample/fact_immigration.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "Has row check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def HasrowCheck(df, table):\n",
    "    print(f'{table} table is under has row check')\n",
    "    result = df.count()\n",
    "    if result == 0:\n",
    "        raise ValueError(f'{table} table has no record')\n",
    "    print(f'{table} table has {result} records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "HasrowCheck(dim_demographic, 'dim_demographic')\n",
    "HasrowCheck(dim_temperature, 'dim_temperature')\n",
    "HasrowCheck(fact_immigration, 'fact_immigration')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Null check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "dim_demographic.createOrReplaceTempView(\"dim_demographic\")\n",
    "dim_temperature.createOrReplaceTempView(\"dim_temperature\")\n",
    "fact_immigration.createOrReplaceTempView(\"fact_immigration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def nullCheck(table_dict):\n",
    "    \n",
    "    for table, column in table_dict.items():\n",
    "        print(f'{table} table is under null check')\n",
    "        records = spark.sql(f'SELECT * FROM {table} WHERE {column} IS NULL')\n",
    "        if records.count() > 0:\n",
    "            raise ValueError(f'Null value found in {table}.{column}')\n",
    "        print(f'{table} table passed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "table_dict = {'dim_demographic': 'cicid',\n",
    "              'dim_temperature': 'i94port',\n",
    "              'fact_immigration': 'immig_id'}\n",
    "\n",
    "nullCheck(table_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Duplicates check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def duplicatesCheck(df, table, column):\n",
    "    print(f'{table} table is under duplicates check')\n",
    "    result = df.count()\n",
    "    if result > df.dropDuplicates([column]).count():\n",
    "        raise ValueError(f'{table} table has duplicates')\n",
    "    print(f'{table} table passed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "duplicatesCheck(dim_demographic, 'dim_demographic', 'cicid')\n",
    "duplicatesCheck(dim_temperature, 'dim_temperature', 'i94port')\n",
    "duplicatesCheck(fact_immigration, 'fact_immigration', 'immig_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.3 Examples of query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# If we want to check the most visited cities, this query gives the answer\n",
    "most_visited_city = spark.sql('''\n",
    "                              SELECT COUNT(*) cnt, T.city\n",
    "                              FROM fact_immigration I JOIN dim_temperature T\n",
    "                              ON I.i94port = T.i94port\n",
    "                              GROUP BY T.City\n",
    "                              ORDER BY cnt DESC\n",
    "                              LIMIT 10\n",
    "                              ''')\n",
    "\n",
    "most_visited_city.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# If we want to check the countries the most people come from, this query gives the answer\n",
    "home_countries = spark.sql('''\n",
    "                           SELECT COUNT(*) cnt, D.origin_country\n",
    "                           FROM fact_immigration I JOIN dim_demographic D\n",
    "                           ON I.cicid = D.cicid\n",
    "                           WHERE D.origin_country IS NOT NULL\n",
    "                           GROUP BY D.origin_country\n",
    "                           ORDER BY cnt DESC\n",
    "                           LIMIT 10\n",
    "                           ''')\n",
    "\n",
    "home_countries.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Our purpose of this data model is to check the correlation between temperature and the number of visits\n",
    "corr_table = spark.sql('''\n",
    "                       SELECT COUNT(*) cnt, average_temp\n",
    "                       FROM fact_immigration\n",
    "                       GROUP BY average_temp\n",
    "                       ''')\n",
    "\n",
    "corr_table.toPandas().corr()\n",
    "# It turned out that these two factors have correlation coefficient of 0.1 which is weak correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.4 Data dictionary "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### dim_demographic\n",
    "* cicid: id from sas file\n",
    "* i94port: 3 charactor code of destination city\n",
    "* i94yr: numeric year\n",
    "* i94mon: numeric month\n",
    "* i94cit: 3 numeric digit code of origin country\n",
    "* arrdate: date of arrival to USA\n",
    "* depdate: date of departure from USA\n",
    "* i94visa: purpose of immigration (1=Buisness, 2=Pleasure, 3=Student)\n",
    "* origin_country: origin country based on i94cit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### dim_temperature\n",
    "* i94port: 3 charactor code of destination city\n",
    "* City: city name\n",
    "* Country: country name\n",
    "* Latitude: latitude\n",
    "* Longitude: longitude\n",
    "* average_temp: average temperature of each city"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### fact_immigration\n",
    "* immig_id: automatically created id\n",
    "* i94port: 3 charactor code of destination city\n",
    "* cicid: id from sas file\n",
    "* average_temp:\n",
    "* i94yr: numeric year\n",
    "* i94mon: numeric month\n",
    "* i94cit: 3 numeric digit code of origin country\n",
    "* arrdate: date of arrival to USA\n",
    "* depdate: date of departure from USA\n",
    "* i94visa: purpose of immigration (1=Buisness, 2=Pleasure, 3=Student) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "* **Clearly state the rationale for the choice of tools and technologies for the project.**</br>\n",
    "The tool we used in this project is apache spark. Spark is easy to use and fast when to handle a huge amount of data.\n",
    "It also can perform advanced analytics fast.\n",
    "* **Propose how often the data should be updated and why.**</br>\n",
    "Taking a look for dataset, month is the smallest time unit. So data should be updated monthly.\n",
    "* **Write a description of how you would approach the problem differently under the following scenarios:**\n",
    " * **The data was increased by 100x.**</br>\n",
    " If the amount of data is much bigger than now it is, instead of local machine, better to use AWS cloud and redshift cluster which is optimized for running big data\n",
    " * **The data populates a dashboard that must be updated on a daily basis by 7am every day.**</br>\n",
    " It had be better use apache airflow that we can manage the schedule.\n",
    " * **The database needed to be accessed by 100+ people.**</br>\n",
    " Datesets should be stored in S3bucket as parquet files, so users can access externally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
